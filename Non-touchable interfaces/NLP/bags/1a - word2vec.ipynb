{"cells":[{"cell_type":"markdown","metadata":{"id":"Ue5hxxkdAQJg"},"source":["<img src=\"https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Word2vect\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kCED1hh-Ioyf"},"outputs":[],"source":["import sys\n","import numpy as np\n","import pandas as pd\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PUbfVnzIIoMj"},"outputs":[],"source":["def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"]},{"cell_type":"markdown","metadata":{"id":"DMOa4JPSCJ29"},"source":["### Datos"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RIO7b8GjAC17"},"outputs":[],"source":["corpus = np.array(['que dia es hoy', 'martes el dia de hoy es martes', 'martes muchas gracias'])"]},{"cell_type":"markdown","metadata":{"id":"8WqdaTmO8P1r"},"source":["Documento 1 --> que dia es hoy \\\n","Documento 2 --> martes el dia de hoy es martes \\\n","Documento 3 --> martes muchas gracias"]},{"cell_type":"markdown","metadata":{"id":"FVHxBRNzCMOS"},"source":["### 1 - Obtener el vocabulario del corpus (los términos utilizados)\n","- Cada documento transformarlo en una lista de términos\n","- Armar un vector de términos no repetidos de todos los documentos"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3ZqTOZzDI7uv"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'es', 'martes', 'de', 'gracias', 'el', 'dia', 'hoy', 'que', 'muchas'}\n","\n","[list(['que', 'dia', 'es', 'hoy'])\n"," list(['martes', 'el', 'dia', 'de', 'hoy', 'es', 'martes'])\n"," list(['martes', 'muchas', 'gracias'])]\n"]}],"source":["def listyfy( corpus ): \n","    return np.char.split(corpus)\n","\n","def dictionarify(corpus): \n","    dictionary = set()\n","\n","    for document in corpus:\n","        dictionary.update(document)\n","    return dictionary\n","\n","\n","listed_corpus = listyfy( corpus )\n","dict_corpus = dictionarify(listed_corpus)\n","\n","\n","\n","print(dict_corpus)\n","print(\"\")\n","print(listed_corpus)\n"]},{"cell_type":"markdown","metadata":{"id":"RUhH983FI7It"},"source":["### 2- OneHot encoding\n","Data una lista de textos, devolver una matriz con la representación oneHotEncoding de estos"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Os0AAQo6I6Z1"},"outputs":[{"name":"stdout","output_type":"stream","text":["['martes', 'muchas', 'gracias']\n","\n","{'es', 'martes', 'de', 'gracias', 'el', 'dia', 'hoy', 'que', 'muchas'}\n","\n","[[1. 0. 0. 0. 0. 1. 1. 1. 0.]\n"," [1. 1. 1. 0. 1. 1. 1. 0. 0.]\n"," [0. 1. 0. 1. 0. 0. 0. 0. 1.]]\n"]}],"source":["def onehotify (listed_corpus=listed_corpus , dict_corpus=dict_corpus):\n","    output = np.zeros((corpus.shape[0],len(dict_corpus)))\n","\n","    for j, document in enumerate(listed_corpus):\n","        for i, term in enumerate(dict_corpus) :        \n","            if (document.count(term)>0) : output[j,i]= 1\n","            else : output[j,i]= 0\n","    \n","    return output\n","\n","#ejemplo\n","\n","onehot_corpus = onehotify(listed_corpus,dict_corpus)\n","\n","print(listed_corpus[2])\n","print(\"\")\n","print(dict_corpus)\n","print(\"\")\n","print(onehot_corpus)"]},{"cell_type":"markdown","metadata":{"id":"IIyWGmCpJVQL"},"source":["### 3- Vectores de frecuencia\n","Data una lista de textos, devolver una matriz con la representación de frecuencia de estos"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"yqij_7eHJbUi"},"outputs":[{"name":"stdout","output_type":"stream","text":["['martes', 'muchas', 'gracias']\n","\n","{'es', 'martes', 'de', 'gracias', 'el', 'dia', 'hoy', 'que', 'muchas'}\n","\n","[[1. 0. 0. 0. 0. 1. 1. 1. 0.]\n"," [1. 2. 1. 0. 1. 1. 1. 0. 0.]\n"," [0. 1. 0. 1. 0. 0. 0. 0. 1.]]\n"]}],"source":["def frecuencify (listed_corpus=listed_corpus , dict_corpus=dict_corpus):\n","    output = np.zeros((corpus.shape[0],len(dict_corpus)))\n","\n","    for i, term in enumerate(dict_corpus) :\n","        for j, document in enumerate(listed_corpus):\n","\n","            output[j,i]= document.count(term)\n","    \n","    return output\n","\n","#ejemplo            \n","print(listyfy(corpus)[2])\n","print(\"\")\n","print(dictionarify(listyfy(corpus)))\n","print(\"\")\n","print(frecuencify( listyfy(corpus) , dictionarify(listyfy(corpus))))"]},{"cell_type":"markdown","metadata":{"id":"z_Ot8HvWJcBu"},"source":["### 4- TF-IDF\n","Data una lista de textos, devolver una matriz con la representacion TFIDF"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"waG_oWtpJjRw"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>es</th>\n","      <th>martes</th>\n","      <th>de</th>\n","      <th>gracias</th>\n","      <th>el</th>\n","      <th>dia</th>\n","      <th>hoy</th>\n","      <th>que</th>\n","      <th>muchas</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.405465</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.405465</td>\n","      <td>0.405465</td>\n","      <td>1.098612</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.405465</td>\n","      <td>0.810930</td>\n","      <td>1.098612</td>\n","      <td>0.000000</td>\n","      <td>1.098612</td>\n","      <td>0.405465</td>\n","      <td>0.405465</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.405465</td>\n","      <td>0.000000</td>\n","      <td>1.098612</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.098612</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         es    martes        de   gracias        el       dia       hoy  \\\n","0  0.405465  0.000000  0.000000  0.000000  0.000000  0.405465  0.405465   \n","1  0.405465  0.810930  1.098612  0.000000  1.098612  0.405465  0.405465   \n","2  0.000000  0.405465  0.000000  1.098612  0.000000  0.000000  0.000000   \n","\n","        que    muchas  \n","0  1.098612  0.000000  \n","1  0.000000  0.000000  \n","2  0.000000  1.098612  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["def find_idf( listed_corpus , dict_corpus ):\n","    idf_vector = np.zeros([len(dict_corpus),])\n","\n","    one_hot_temp = onehotify(listed_corpus,dict_corpus)\n","    vsum_one_hot_temp = np.sum(one_hot_temp,axis=0)\n","    \n","    for idx in range(len(idf_vector)):\n","        idf_vector[idx] = np.log( listed_corpus.shape[0] / vsum_one_hot_temp[idx])\n","\n","\n","    return idf_vector\n","\n","  \n","\n","def get_TFIDF( listed_corpus , dict_corpus ):\n","    idf = find_idf(listed_corpus=listed_corpus , dict_corpus=dict_corpus)\n","    tf = frecuencify(listed_corpus=listed_corpus , dict_corpus=dict_corpus)\n","\n","    TFIDF = tf * idf  #no necesita transpuesta porque es unidimensional.\n","\n","    return TFIDF\n","\n","# hago un dataframe para que sea pretty la forma en que se muestra el dataset\n","df = pd.DataFrame(get_TFIDF( listed_corpus , dict_corpus ), columns = list(dict_corpus))\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"xMcsfndWJjm_"},"source":["### 5 - Comparación de documentos\n","Realizar una funcion que reciba el corpus y el índice de un documento y devuelva los documentos ordenados por la similitud coseno"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"CZdiop6IJpZN"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1.        0.2003419 0.       ]\n"]}],"source":["#Como la evaluacion del coseno necesita numeros y no texto, voy a usar alguna de las representaciones de word2 vec realizadas\n","# de manera random, voy a usar la tf-idf, porque escuche que se ha usado mucho. \n","\n","def how_similar(corpus , corpus_doc_idx):\n","    similarty_indexes = np.zeros([corpus.shape[0],])\n","\n","    listed_corpus = listyfy( corpus )\n","    dict_corpus = dictionarify(listed_corpus)\n","    \n","    TDFIDF_word2vec = get_TFIDF( listed_corpus , dict_corpus )\n","\n","    for idx in range(len(TDFIDF_word2vec)):\n","        similarty_indexes[idx] = cosine_similarity(TDFIDF_word2vec[corpus_doc_idx] , TDFIDF_word2vec[idx] )\n","\n","    return similarty_indexes\n","\n","\n","Compare_word2ved_corpus = how_similar(corpus=corpus, corpus_doc_idx=0)\n","\n","\n","print(Compare_word2ved_corpus.T)\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["As expected, the similarity of the first vectorized document with itself is 1. and what it means is that vectors are colinear. and that every component is the same on both vectorized documents is the same. \n","\n","But, that doesn't mean that the original documents where equal, just that their vectorized representations are.  "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["0.9285714285714286"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["a=[0,1,2,3]\n","b=[0,2,1,3]\n","\n","cosine_similarity(a, b )"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5fRYTpympAwJSVbric6dW","collapsed_sections":[],"name":"1a - word2vec.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 ('NLP')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"db02c8c4514aa220eff79aceef1a08987b492a1ceac9f4c924fdffa694c5d5c2"}}},"nbformat":4,"nbformat_minor":0}
